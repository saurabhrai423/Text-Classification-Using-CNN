# -*- coding: utf-8 -*-
"""atul.bunkar04@gmail.com_20.ipynb

Automatically generated by Colaboratory.


"""## Modelling !"""

from numpy import array
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences


from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D , Dropout
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding , Concatenate
from tensorflow.keras.models import Model
import pickle
from tensorflow.keras.utils import to_categorical
import tensorflow as tf

import pandas


"""#### Model 1"""

import random  as rn
tf.keras.backend.clear_session()

np.random.seed(0)
rn.seed(0)

input = Input(shape=(max_length,), dtype='int32')

embedded_layer =  Embedding(vocab_size , 100 , weights=[embedding_matrix] , input_length= max_length,  trainable=False)(input)

x = Conv1D(64 , 3, activation='relu' ,padding='same' )(embedded_layer)
y = Conv1D(64 , 4, activation='relu',padding='same' )(embedded_layer)
z = Conv1D(64 , 5, activation='relu' ,padding='same' )(embedded_layer)

mx = Model(input, x)
my = Model(input, y)
mz = Model(input, z)

concat1 = Concatenate()([mx.output, my.output, mz.output])
pool1 = MaxPooling1D()(concat1)

i =  Conv1D(64 , 5, activation='relu' ,padding='same' )(pool1)
j =  Conv1D(64 , 6, activation='relu',padding='same' )(pool1)
k =  Conv1D(64 , 4, activation='relu' ,padding='same' )(pool1)

mi = Model(input, i)
mj = Model(input, j)
mk = Model(input, k)

concat2 = Concatenate()([mi.output, mj.output, mk.output])
pool2 = MaxPooling1D()(concat2)

p = Conv1D(128 , 4, activation='relu' ,padding='same' )(pool2)

flat = Flatten()(p)

drop = Dropout(0.5)(flat)
FC = Dense(128, activation='relu')(drop)

out = Dense(20, activation='softmax')(FC)

model = Model(input, out)

tf.keras.utils.plot_model(model , show_shapes = True , to_file= 'm1.png' )

# CallBacks 
from sklearn.metrics import f1_score
from tensorflow.keras.callbacks import EarlyStopping

class m_F1(tf.keras.callbacks.Callback):
  def __init__(self, x_val , y_val):  
        self.x_val = x_val
        self.y_val = y_val
  def on_train_begin(self, logs={}):
        
    self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []  , 'microf1' : [] }
    
  def on_epoch_end(self, epoch, logs={}):

    y_pred = self.model.predict(self.x_val)
    y_pred1 = np.argmax(y_pred, axis=1)

    y_test1 = np.argmax(self.y_val, axis = 1)

    f1 = f1_score(y_test1, y_pred1.round() , average='micro')

    print(f' f1 score %0.3f' % ( round(f1, 5)))


    self.history['loss'].append(logs.get('loss'))
    self.history['acc'].append(logs.get('acc'))
    if logs.get('val_loss', -1) != -1:
        self.history['val_loss'].append(logs.get('val_loss'))
    if logs.get('val_acc', -1) != -1:
        self.history['val_acc'].append(logs.get('val_acc'))
        
    self.history['microf1'].append(f1)

!rm -rf ./logs1 
earlystop = EarlyStopping(monitor='val_acc', min_delta=0.1, patience=1)
micro_f1 = m_F1(X_te_pad , y_test)
log_dir="logs1"
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)


cb = [earlystop ,micro_f1 , tensorboard] 

model.compile(loss='categorical_crossentropy',  
              optimizer='Adam',
              metrics=['acc'] )

model.fit(X_tr_pad, y_train,
          batch_size= 32,
          epochs=4,
          validation_data=(X_te_pad, y_test) , callbacks = cb)


